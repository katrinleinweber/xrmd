<!-- 
pandoc --filter pandoc-citeproc --bibliography /Users/sharper/Active/Papers/MethodBoxDS/JMIR/citeulike-library.bib  --csl /Users/sharper/Active/Papers/MethodBoxDS/JMIR/journal-of-medical-internet-research.csl  MethodBoxDS.md -o mbr.html
	
LaTeX Title:  Data Science Needs Single-Field Natural Language Search Interfaces for Variable Search.
LaTeX Author: Simon Harper  
        Caroline Jay 
Base Header Level: 3
latex mode: memoir
latex input mmd-memoir-begin-doc
latex footer: mmd-memoir-footer
Date:   April 10, 2015  

-->



# Abstract
Data Discovery, particularly the discovery of key variables and their inter-relationships is key to secondary data analysis, and in-turn, the evolving field of Data Science. Current interfaces make a presumption that their users are experts within the domain, and, therefore, provide complex advanced interfaces to support these 'experts'. These interfaces hark back to a time when searches needed to be accurate first time as there was a high computational cost associated with each search. However, the cross-disciplinary nature of Data Science can make no assumptions as to the domain expertise of a particular scientist, whose interests may intersect many domains. In this case, scientists performing secondary data analysis share more in common with the 'Google Generation' than with their single-domain, single-tool forebears. Here we report the results of a study evaluating two user interfaces that provide access to survey data stored in the UK Data Archive (UKDA). One user interface is based on a 'Google-like' Web Search interface that enables users to browse, search for and view metadata for individual factors and variables; the other provides a Traditional Search user interface. We find that Web Search interfaces fit variable data discovery needs and expectations better than the traditional, because they are consistent with modern search interfaces and are, therefore, familiar across domains. Further, they allow queries to be refined as the search proceeds and acknowledge serendipity as part of this refinement. The results provide strong evidence that Data Science should adopted single-field natural language search interfaces for variable search supporting in particular: query reformulation; data browsing; faceted search; surrogates; relevance feedback; summarisation, analytics and visual presentation. 

# Introduction

Data Science spans many domains from the biological sciences, medical informatics, health care, social sciences and the humanities. Further, it heavily influences economics, business and finance. The 'Big data' or 'Broad data' used in Data Science is extensively used for  secondary analysis (using existing data to answer new research questions)  due to its size, cost, and the specialists tools needed in its gathering. Indeed, this data reuse is critical for both application and data mashups and acknowledges the cross-disciplinary of the gathered data and its importance in combinatorial use? In this case Data Discovery, particularly the discovery of key variables and their inter-relationships is key to finding and understanding data. Further, in the cross-disciplinary area of Data Science we can make no assumptions as to the domain expertise of a particular data scientist, whose interests may intersect many domains. 

Secondary data analysis has a number of key functions [@law2005reduce] in relation to Data Science: it allows researchers to link datasets to answer questions that the files could not address adequately in isolation [@Fienberg:1985aa]; it creates opportunities to explore associations between factors that were not anticipated at the time of data collection [@Dale1988Doing]; and it has value from an ethical perspective by increasing the potential benefits to society arising from public investment in the collection of the original data [#law2005reduce]. We can see that secondary data analysis is essential to many areas of science and policy research, but it is often impeded by difficulties in data discovery; finding the most appropriate data to use for analysis can be problematic. Typically, the researcher needs to find a handful of appropriate variables among collections of thousands, often spread across multiple datasets such as successive years of a repeated survey. Current data archive information systems do not optimally support this search process; indeed they make a presumption that their users are experts within the domain, and, therefore, provide complex advanced interfaces to support these 'experts'. These interfaces hark back to a time when searches needed to be accurate first time as there was a high computational cost associated with each search. In this case, data scientists share more in common with the `Google Generation' than with their single-domain, single-tool forebears. 

Anecdotally, following Web Search interface design (best expressed by 'Google', 'Bing', 'Ask', etc.) would seem like best practice, but there is little empirical evidence to support such a claim. While the need to improve access to data for research purposes is recognised [#Esrc2009National], no studies to date have directly examined how the user interface of tools providing access to archives impacts on the researcher's ability to discover and extract relevant data. Here, we report the results of a study conducted in collaboration with the [UK Data Archive (UKDA)](http://www.data-archive.ac.uk/)[^esds], the largest collection of digital research data in the social sciences and humanities in the UK. 

[^esds]: At the time of the study, access to data stored in the UKDA -- including Government and other large-scale surveys -- was formally provided by the Economic and Social Data Service (ESDS - http://www.esds.ac.uk), a data archiving and dissemination service supporting the secondary use of data in both research and teaching. ESDS provided access to a wealth of data and had more than 250 research institutions registered to use its services. 

Searching and accessing data from within this archive was not a trivial process, however, due to two key issues [#Thew2010MethodBox]. Firstly, researchers had to work out which of the more than 5,000 datasets stored in the UKDA could most appropriately be used to answer their research question. Getting a sufficient overview of what was available in each set was difficult, and researchers often picked certain datasets simply because they were familiar with them [#freese2009secondary]. Identifying the appropriate variables within a dataset was a second problem. Surveys typically contain hundreds, if not thousands of variables (the Health Survey for England 2007, for example, contains more than 2,000), and variable labels may not obviously reflect their content. To accurately identify variables of interest, the researcher must read the original questionnaire alongside supporting documentation, a process that can take days or weeks of work, and which may ultimately be fruitless: until the researcher has completed the process they do not necessarily know if the dataset can answer their research question. Whilst fully understanding a dataset and reading its documentation is important to the research process, it would save researchers a great deal of time if they could limit this in-depth exploration to datasets that were likely to be useful to them. Understanding other aspects of data use, such as how derived variables have been constructed, or how data from a number of years can be compared, is also problematic. 

Current systems can be thought of as divided into two categories: (1) those which use a traditional advanced search interface [#6182576], which expects accurate queries, patient users, moderated and homogeneous data; and, (2) those which use a Web Search interface which expects vague queries, impatient users and an enormous and rapidly expanding collection of unmoderated, heterogeneous data [#smyth2004exploiting]. We suggest that variable search for secondary analysis has more in common with the hostile search environment of the modern Web than it does with Traditional Search. 

In this case, our experiment compares one interface based on a 'Google-like' Web Search interface that enables users to browse, search for and view metadata for individual factors and variables; the other provides a traditional advanced search user interface (which presumes the user knows what they are looking for). Although more data archives do now have this kind of interface, our work is important because there is very little empirical work looking at the benefits of the search interface versus traditional advanced search interfaces.

Our controlled evaluation compares the effectiveness and efficiency of these two competing approaches to variable data discovery; Traditional Advanced Search against Web Search. We find that Web Search interfaces fit variable data discovery needs and expectations better than the traditional, because they are consistent with modern search interfaces and are, therefore, familiar across domains. Further, they allow queries to be refined as the search proceeds and acknowledge serendipity as part of this refinement. 

The results provide strong evidence that DataScience should adopted single-field natural language search interfaces for variable search, conforming to ''Marchionini's Human-Computer Information Retrieval'' (HCIR) framework [#marchionini2006toward] and in particular supporting: query reformulation; data browsing; faceted search; surrogates; relevance feedback; summarisation, analytics and visual presentation. 

# Background

There are numerous Websites that provide access to the results of large-scale surveys (for example the [Office for National Statistics](http://www.ons.gov.uk) in the UK, [Eurostat](http://epp.eurostat.ec.europa.eu) in the EU and the [Bureau of Labor Statistics](http://www.bls.gov/data/) and [Inter-University Consortium for Political and Social Research (IPCSR)](http://www.icpsr.umich.edu) in the US. Until recently, the majority of survey repositories primarily used Traditional Search for the discovery of entire datasets, although the inclusion of Web Search interfaces for variable data discovery is becoming more common. Both the IPCSR Website and the [Rand Survey Metadata Repository](https://mmicdata.rand.org/megametadata/), provide access to a number of quantitative surveys conducted around the world and offers a facility for searching datasets at the level of variables. [As detailed in Section](#impact), following this study the UKDA also now supports variable data discovery using a Web Search interface.

## Traditional Search Interfaces

Traditional Advanced Search, and the interfaces which facilitate it are based on a number of long-held premises. The most noteworthy -- in this context -- are the presumptions that the interface can expect accurate queries, that users are patient, and that the data will be moderated and homogeneous [#6182576]. In some cases, especially within the scientific research domain, these assumptions hold true. In other cases, however, they do not reflect reality. This seems especially to be the case with regard to searches of variable data sets which seem to have more in common with the heterogeneity of the open Web. Increasingly, Traditional Search interfaces focused on delivering well-curated datasets (often already known to the user) are now looking for novel ways to fill the user expectation gap [#TorresParejo20135448]. These systems are increasingly recognising that providing access to relevant information adapted to the needs and the context of the user is a real challenge [#978-3-642-32498-7_19] and that contextual results are becoming more important. Furthermore, evidence suggests that the Traditional Search model predicated on users searching for particular information, the so-called 'information need', may not be as important as navigational searches [#Broder:2002:TWS:792550.792552]. Indeed, understanding the underlying goals of user searches is becoming increasingly important; for instance the previously unexplored 'resource-seeking' [#Rose:2004:UUG:988672.988675] goal may account for a larger fraction of Web Searches than previously thought.

Traditional Search expects the user to have well-defined boundaries for the information they are looking for, along with a good knowledge of the terms and meta-information that may be used to describe that information. This is increasingly not the case, especially in the context of variable data discovery and user-centred approaches [#10.1007/978-3-642-36415-0_3] so common in the broad domain of Data Science. 

## Web Search Interfaces

Web Search, and its offshoot of HCIR, recognise the deficiencies in the Traditional Search model, and thus expect vague queries, impatient users and an enormous and rapidly expanding collection of unmoderated, heterogeneous data [#smyth2004exploiting]. Indeed, the model of Traditional Search is changing, with the widespread use of Web Search engines, employment of simple queries, and decreased viewing of results pages -- changes that have resulted from algorithmic enhancements by Web Search engine companies [#Jansen2006248]. Large providers, such as Google, run around 10,000 experiments per year in an attempt to refine both the search engine, and the search interface and interactivity [#Varian2012Statistics]. We could arguably conclude that this tempo of experimentation makes these kinds of search engines de-facto best practice for all other search instruments -- the latter not being able to match the former's ability to adapt and refine their algorithms and interactions; a trend we can see in search result clustering [#gong2013interactive], for instance.

It is therefore not surprising that about 85% of Internet users surveyed claim to use search engines and search services to find specific information [#Kobayashi:2000:IRW:358923.358934]. These users have expectations that bleed from Web Search into all other areas which require search. To a naive user, all search activity is the same [#doi:10.1061/(ASCE)CP.1943-5487.0000229]. In this case, we suggest that variable search for secondary analysis has more in common with the extremely hostile search environment of the modern Web than it does with Traditional Search.

## Faceted Search and the Google Generation

The move from Traditional Search to Web Search may be a result of changes in user attitudes and needs. 'The Google Generation' appears to behave very differently to older generations [#rowlands2008google]. They are less confident about their searching prowess, demonstrated by the fact that they viewed fewer pages, visited fewer domains and undertook fewer searches than older users [#zhang2008undergraduate]. Also, tellingly, their search statements were much more the product of cut and paste. These characteristics -- of relying less on working memory and demonstrating lower competence at multi-tasking -- has knock-on implications for researching in an online environment [#nicholas2010behaviour][#spring2010health]. To overcome some of these limitations, we have seen a rise in faceted search, which combines query and browse strategies interactively providing an iterative way to refine search results [#Zhang:2012:FSH:2232817.2232924]. Faceted search allows users to start very generally and then iteratively refine their searches by allowing them to selectively apply multiple filters. These filters can be based on taxonomies [#Wei:2013:SFS:2481562.2481564], simple classifications systems [#Zhang:2012:FSH:2232817.2232924], or other spatial locations [#MEET:MEET14504901328] -- in some cases they are generated from search results sharing some common overlap [#Wei:2013:SFS:2481562.2481564]. This faceted approach dovetails into the evolving behaviours of the Google Generation, and assists in complex decision-making [#ASI:ASI21647]

## Beyond Web Search

For reasons ranging from an obligation to curiosity, Web Search is now moving beyond the individual and into the social [#mislove2006exploiting]. Users have a strong inclination to seek information from others during the search process. Indeed, search systems using statistical analytics over traces left behind by others can help support the search experience [#chi2009information]. Furthermore, result clustering based on social networks in a crowdsourcing role [#Horowitz:2010:ALS:1772690.1772735], and grouped clusters displaying multiple tabbed search results [#Zamir19991361] are also becoming increasingly used. These advances suggest a social component to dataset and variable retrieval will, in the future, be expected.

![Figure 1.1: Simple Search -- Can be seen on the right-hand side.](Figures/ESDS_home.jpg "Figure 1.1: Hello")

[Figure 1.1]: Figures/ESDS_home.jpg "Figure 1.1: Simple Search"


# Context of this Study

Access to the UKDA via ESDS was set up primarily to facilitate the discovery and download of entire datasets, and as such shares much with traditional advanced search interfaces. The system provided several ways in which users could access individual variable descriptions, including a dedicated variable search facility, but anecdotal evidence indicated that these were difficult to use, and not an adequate substitute for reading the complete survey documentation. Recognising these issues, the UKDA decided to work with the University of Manchester (UK), to develop a [Web Search interface](http://www.methodbox.org) as part of the [ESRC-funded Obesity eLab](http://www.mygrid.org.uk/projects/obesity-e-lab/) project.

This interface was designed to simplify the process of accessing survey data, by enabling people to look for variables of interest through a familiar, and potentially more suitable, interface. Researchers typed a query into a single search box and then browsed relevant results. Variables were displayed in a tabular format, with the description shown prominently, allowing users to see at a glance whether the variable was relevant to their research question.

Although there is a large body of research examining search behaviour in Information Retrieval [#markey2007twenty],[#robertson2008history], there is little that directly examines, from a user's perspective, how best to retrieve variable data from archived surveys. Our solution, called MethodBox, initially emerged from the need to understand HCIR as it related to variable data discovery. A requirements analysis was conducted to understand the difficulties users experienced with the existing Traditional Search interface to UKDA, and to pinpoint new features that would help users to identify variables and datasets that could be used to answer research questions. The 'MethodBox' Web Search user interface was then designed to make the search process as straightforward as possible for novice users, reflecting the fact that most of their Information Retrieval experience will have come from the Web [#rowlands2008google],[#novotny2004don].

![Figure 1.2: Orientation -- The search form is at the top of the page and the results are returned underneath. To view the variables, the user must click the 'Study Description/Documentation', then use the 'Variable List' link at the top of the page.][Figure 1.2]

[Figure 1.2]: Figures/ESDS_cs.png "Figure 1.2: Orientation" width="75%"

## Current Traditional Search Interface
At the time of the study, access to data stored in the UKDA -- including Government and other large-scale surveys -- was formally provided through a Website hosted by the ESDS, which provided numerous facilities for searching the UKDA catalogues. 

![Figure 1.3: BSAS 2009 -- The variable list in the British Social Attitudes Survey 2009. To view a variable, the user selects one from the list box and clicks 'show variable'.][Figure 1.3]

[Figure 1.3]: Figures/ESDS_vl.png "Figure 1.3: BSAS 2009" width="75%"

On the home page (see Figure 1.1), the Simple Search allowed users to search all fields in a record for keywords or phrases. 

The resulting surveys were listed on the Catalogue Search page (see Figure 1.4), and searches could then be refined using the Catalogue Search form. To access the variables in the survey, the user clicked through to the 'Survey Description/Documentation', and then followed the 'Variable List' link at the top of the page, which provided a list of all the variables in the dataset (see Figure 1.3). Variable details were provided on a separate page when the user selected the name and clicked 'show variable'. The Variable Search (see Figure 1.5) contained a single search box, and returned surveys that contain variables matching the keywords in a list underneath. Users could click through to the survey description and view the variable list as before, or click the link on the left of the result to go straight to the list of variables. The Nesstar tool allowed users to search and browse surveys in a tree view (see Figure 1.6), and the ESDS Government Variable Search returned a list of variables that matched search terms just from the Government surveys. 

![Figure 1.5: Variable Orientation -- The search box is at the top of the page, and the results are returned underneath. To view the list of variables, the user clicks the 'Variables in...' link on the left hand side of each result, which provides a list of all the variables in the dataset.][Figure 1.5]

[Figure 1.5]: Figures/ESDS_vs.png "Figure 1.5: Variable Orientation" width="75%"

In addition to the search facilities, there were numerous routes through which users could browse the available surveys, such as the 'Browse by subject' and 'Major studies' pages. Lists of variables could then be accessed from the study description pages. The ESDS Website, like many sites, was frequently edited and upgraded; the study was conducted between the 27th September and 3rd November 2011, a period during which there were no major changes to the functionality offered by the site. 

![Figure 1.6: The Nesstar interface -- Surveys matching the search terms are listed in the menu on the left hand side.][Figure 1.6]

[Figure 1.6]: Figures/nesstar.png "Figure 1.6: The Nesstar interface" width="75%"

## Comparable Web Search Interface

The Web Search interface (MethodBox), developed as part of the ESRC-funded Obesity eLab project, was designed to simplify the process of accessing survey data by enabling people to look for variables of interest through a straightforward 'Web Search' interface embedded in a scientific social network. Researchers typed a query in a single search box and then browsed relevant results. Variables were displayed in a table format, which could be reordered according to a number of categories. The variable description was displayed prominently, allowing users to see at a glance whether the variable was relevant to their research question. Variables of interest could be selected and then downloaded to the user's desktop. 

![Figure 1.7: MethodBox Home -- Users type a query in the central search box, and can modify what is searched (surveys, variables etc.) using the tick boxes underneath.][Figure 1.7]

[Figure 1.7]: Figures/mb_home.png "Figure 1.7: MethodBox Home" width="75%"

A clear priority identified in the requirements analysis was a fast and straightforward means of identifying variables that are relevant to a particular research question. To achieve this, MethodBox assimilates all the required information about a variable, including its name, values and metadata, using the [Document Data Initiative (DDI)](http://www.ddialliance.org) XML files available through the ESDS Nesstar service, and through processing the dataset documentation with the [Utopia PDF parser](http://getutopia.com). This process allowed MethodBox to treat variables as first class citizens in their own right. Users could also upload their own data files and add metadata in the DDI XML format. Assets inside MethodBox were indexed using Apache Solr, allowing users to search variable names and metadata quickly and easily, as well as the surveys, data analysis scripts, data extracts (subsets of variables created by other users), publications and user profiles also held by MethodBox. 

![Figure 1.8: Survey Results -- Categories that contain results are displayed in separate tabs. Results are displayed in a table and ordered according to relevance, but can be sorted by clicking the table headers.][Figure 1.8]

[Figure 1.8]: Figures/mb_results.png "Figure 1.8: Survey Results" width="75%"

The MethodBox user interface was designed to correspond to the common mental model of an online Web Search interface: a box for entering terms; a button to run the search; and a list of results [#zhang2008undergraduate]. The homepage consisted of a single, 'Google-style' search box, with tick boxes underneath to allow users to specify what they wanted to search (see Figure 1.7). All categories (surveys, variables, methods, data extracts and publications) were selected by default. Matching results were returned in a table format. Results were initially ordered according to relevance, but could be sorted -- for example by year or survey -- by clicking the table headers. If there were matching results in more than one category, these were displayed in separate tabs (see Figure 1.8). Variable details could be accessed by clicking the arrow to the left of the result, which provided them in a drop down box, or by clicking the variable title, which showed them on a separate page. Users could also select and search a subset of surveys (see Figure 8), or navigate to a complete list of the variables from a link on the survey description page. If users were logged in, they could add any number of variables to their 'shopping' cart, before downloading this subset of data to their desktop as a 'data extract'. 

![Figure 1.9: Surveys -- Users can select surveys and search them for variables.][Figure 1.9]

[Figure 1.9]: Figures/mb_survey.png "Figure 1.9: Surveys" width="75%"


# Evaluation

The aim of the evaluation was to understand whether the Web Search interface provided more effective, efficient and satisfactory access to variable data stored in the UKDA than the Traditional Search interface. 

## Methodology

As there are numerous ways of accessing variable data using both interfaces, participants started every task on the homepage of the site and were free to navigate around and use resources as they wished. Participants were given a number of focused questions, and asked to find data with which to answer them. Searching through surveys to gain a complete picture of all the data available to answer a question would be very time--consuming, potentially taking days or weeks [#Thew2010MethodBox]. As a proxy measure participants were therefore asked to locate a single variable that provided as complete an answer as possible. All the tasks were completed using both the interfaces, providing a direct comparison between the two.

### Hypothesis

The purpose of the study was to evaluate the Web Search approach, and as such the broad hypothesis was that users would find the process of discovering variable data to be more effective, efficient and satisfactory using the Web Search interface than the Traditional Search interface. As the study provided an empirical comparison of various approaches to finding variable data, however (i.e. neither Website tied users to following a single 'route' to data discovery), it was expected that the qualitative results in particular would help to identify the features and functionality that participants either liked or disliked and the reasons why, thus contributing to future user interface development.

### Experimental Design

A repeated measures design was used where participants searched for data to answer the same four questions using both interfaces. Participants were asked to approach each search 'from scratch': that is, to look for any data with which to answer the question and choose what they thought was most useful, rather than to search only for the name of a specific variable or dataset that they knew would provide the answer. Participants completed all the tasks using one of the interfaces first, then had a short break while they answered questions about the experience they had just had, before completing the tasks in the same order using the other interface. The order of the tasks was varied according to a Latin square. The design was counter-balanced, so for every participant who completed the tasks in a given order using the Traditional Search interface first, another completed the tasks in the same order but using the Web Search interface first. 

### Participants

Five male and 15 female participants between the ages of 18 and 35 took part in the evaluation. All participants were working or studying in the areas of social science or health science, and had some experience of secondary data analysis. Eleven of the participants had one year's experience or less, 5 had 2-3 years' experience and 4 participants had 4 or more years' experience. Participant's previous experience with the particular tools assessed in the evaluation was very limited. One participant had used both MethodBox and ESDS before 'a couple of times', one had used MethodBox once, and five had occasionally used ESDS. It should be noted that the participants who had previously used MethodBox would have encountered an earlier version with a different user interface. Other online resources participants used to look for data included the Office for National Statistics or Casweb (4 participants), Survey Question Bank (two participants), medical databases (4 participants), European Data Centre for Work and Welfare (one participant), and EuroStat (one participant). Five participants worked mainly with data they had collected themselves or which came from colleagues or supervisors. 

### Tasks

The search tasks were developed in collaboration with the [ESDS Government team (ESDSG)](http://www.esds.ac.uk/search/searchHelpESDS.asp) based at the University of Manchester, and were designed to reflect the kind of research questions that people may seek to answer using the survey data stored in UKDA. ESDSG designed the format for the tasks, and the details of each were decided in a discussion involving both the MethodBox and ESDSG teams. Participants were asked to find a variable that could be used to answer the following questions: 

1.	What proportion of people in Scotland believe Jesus was the son of God? (hereafter referred to as the 'belief' task). 
2.	What proportion of people in Wales speak Welsh fluently? (the 'speak Welsh' task). 
3.	What proportion of people in Northern Ireland have a bus link to local shops and services? (the 'bus link' task). 
4.	What proportion of the British population have private health insurance? (the 'health insurance' task).
When they had found a variable that they felt gave a satisfactory answer, they were asked to say so. They were free to stop at any point if they did not think it was possible to find a variable that would answer the question. 

### Metrics and Data Collection

Experimental sessions were audio and video recorded and participants' eye movements and mouse movements / keystrokes were tracked using Tobii Studio Professional software. Task completion times and correctness scores were calculated, and participants' behaviour and comments during the sessions were documented and analysed. Eye tracking data were used to provide insight into situations that could not be understood using the other measures alone; for example to determine whether a participant was ignoring a matching variable that had appeared, or had not seen it.

After each task, participants were asked to rate, on a scale of 1 to 7 (with 1 being 'very' and 7 being 'not at all') how confident they were that they had found a satisfactory answer, and how easy they found it to obtain their answer. 
After they had completed all four tasks using one interface, they were asked to rate, on a scale of 1 to 7 (with 1 being 'very' and 7 being 'not at all'), how easy they found it to learn how to use the interface, how easy it was to find data using the interface, and their overall satisfaction with the interface. They were also asked to state what they liked and did not like about the interface. 

After they had completed the tasks, participants were asked to state which interface they preferred using for finding variable data, and to provide a reason for this. 

### Procedure

Participants completed a consent form and an entry questionnaire about their previous experience of finding quantitative survey data for secondary analysis. They were then asked to consult the appropriate help documentation for the first interface. For the Traditional Search interface, this involved reading the online ['Help on searching the Data Catalogue'](http://www.esds.ac.uk/search/searchHelpESDS.asp) document (participants were directed in particular to the section on searching for variables) and in the case of the Web Search interface, reading the ['About'](https://www.methodbox.org/about) page and watching the help video, participants completed the four search tasks using the first interface, providing confidence and ease of use ratings after each task, and learnability, overall ease of use and satisfaction ratings when they had completed all the tasks. They then repeated this process using the second interface. Finally they stated their preference for either the Web Search or Traditional Search interface, and provided a reason for this.
 
## Results

The Web Search interface provides an alternative view on the data stored in the UKDA, but at present it does not provide access to the same level of data as the Traditional Search interface (for example, census data are not available through the Web Search interface). The questions were designed so relevant answers could be found in the UK government surveys that can be accessed through both the Web Search interface and the Traditional Search interface. Both sites were checked to confirm that at least one variable (the same in each case) containing all the information required to answer each question could be found. As both sites were live and independently updated, it is possible that they contained other, potentially different matches.
 
### Observations

When using the Web Search interface participants started each task on the home page containing the main search box. Just over half the participants used the checkboxes underneath the search box at least once to restrict the search to variables (6 participants) or variables and surveys (7 participants). 

Participants looked for variables by entering terms into either the main search box (in the centre of the home page and at the top of the page throughout the rest of the site) or the variable search box on the survey tab. Thirteen participants chose at least one variable after only a single search; the rest of the time participants performed two or more searches before they found an answer they were happy with. Five participants chose to search within particular surveys at least once. Three participants reordered the results table at least once and seven clicked the 'show all variables' link for a particular survey, although only three ended up choosing a variable from this list, with the rest returning to the search facility. Half the participants looked only at the first page of results before either choosing a variable or searching again; the other half looked beyond the first page for one task (4 participants), two tasks (5 participants) or three tasks (one participant). Observation of participants' eye movements showed that they made a decision about whether or not to view a variable's details primarily by glancing at the 'description' column of the results table.
 
In a number of instances, participants found a variable that answered the question quickly, but did not choose it straightaway. Seven participants hesitated to choose a variable because there were several in the results that would answer the question. In five other cases, the eye tracking data shows that participants saw a correct answer in the first set of search results, but spent some time looking round the page or other parts of the site before choosing the variable as their answer.
 
Three participants failed to find (in their opinion) a satisfactory variable in one task using the Web Search interface, and one failed in two tasks. 

When using the Traditional Search interface, four participants failed to complete one task, two failed to complete two tasks, one failed to complete three tasks and four did not complete any. 

There was a much greater variety in the way that participants used the Traditional Search interface. Four participants used only the variable search and two used only the catalogue search; the remaining participants used a combination of the simple search on the home page (11 participants), the variable search (14 participants) and the catalogue search (13 participants). Eleven participants used more than one search facility within the same task and fourteen used different search facilities across different tasks. Seven participants tried Nesstar, although only one found a satisfactory variable using this tool. Ten participants chose to use the browsing facilities (such as the 'Browse by subject' page) to access study descriptions, in addition to searching. None of the participants accessed the Government variable search, possibly because there were no prominent links to it from the home page or help documentation. 

Seven participants consulted the help when using the Traditional Search interface, compared to two when using the Web Search interface. Six participants used the browser's ````<ctrl + f>```` command at some point to locate text within a page with the Traditional Search interface, whilst only two participants used this approach with the Web Search interface. 

Thirteen participants chose to look beyond the first page of the results following a search. As variables had to be located within a list of all the variables in the dataset, it was typical for participants to spend a long time scrolling before they reached the answer. 

### Performance

The performance measures were the correctness of the results and the time taken to complete the task. 

| Task         | Web Search Interface |  Trad. Search Interface |
|:------------ |----------------------:| ------------------------:|
| Belief       |          2.40 (0.88)|             1.05 (0.94) |
|  Welsh       |           2.45 (0.76)|              1.65 (1.09)|
|  Bus         |           1.95 (1.00)|              0.55 (0.94)|
|  Health      |           2.70 (0.92)|              0.85 (1.23)|
|  **Overall** |       **2.38 (0.89)**|          **1.03 (1.05)**|
HELLO! [Table 1.1: Correctness - Mean (with standard deviation) correctness scores for each task.][Table 1.1]

As the Traditional Search interface provides access to a greater volume of data (and the Web Search interface a subset of this), it is possible that it may contain more relevant variables, increasing the chance that participants may find a correct answer. It is also possible that this may have a negative impact on task completion times, however, as the larger data collection may take longer to search. 

Correctness was scored out of three. If participants found any variable containing all the required information, a score of 3 was given; finding a variable that contained most of the information received a score of 2; finding a variable that contained some of the information received a score of 1; failing to find a relevant variable received a score of 0. Participants were not asked to consider year as part of the search criteria. An investigator from each of the MethodBox and ESDSG teams rated correctness, and reached a consensus about the appropriate value where there was disagreement. 


|  Task         |   Web Search Interface |  Trad. Search Interface
|  -------------|----------------------: |------------------------:
|  Belief       |          159.1 (110.5) |           243.5 (159.1)
|  Welsh        |           143.9 (80.6) |           202.8 (148.9)
|  Bus          |          208.0 (161.5) |           309.8 (153.7)
|  Health       |          163.0 (100.1) |           313.8 (135.9)
|  **Overall**  |      **168.5 (113.2)** |       **267.5 (149.4)**
[Table 1.2: Completion - Mean (with standard deviation) completion times in seconds for each task.][Table 1.2]

Quantitative data were analysed with SPSS 19. Table 1.1 shows the mean correctness values for each task. A task × interface repeated measures GLM procedure shows a main effect of interface, indicating that answers found through the Web Search interface were more likely to be correct (F~1,19~ = 37.3, p < 0.001) and a main effect of task (F~3,57~ = 6.3, p < 0.001), with post hoc pairwise comparisons showing that participants obtained significantly lower scores in the 'bus link' condition than any of the others. There was also a task × interface interaction effect (F~3,57~ = 3.3, p = 0.028), which reflects the fact that whilst correctness scores were lowest for both interfaces in the 'bus link' task, scores for the 'health insurance' task were the second lowest using the Traditional Search interface, but highest using the search engine interface.
 
Observations of the search process show that whilst participants encountered, on aggregate, more than 5 variables that would provide a correct answer in the 'speak Welsh' task, and more than 20 that would answer the 'health insurance' question, in the case of the 'belief' and 'bus link' tasks, all participants who achieved a score of 3 chose the same, single variable, which was the only correct answer to appear during any search. The correctness results for the Web Search interface, which show participants achieved the highest scores for the 'health insurance' task, followed by the 'speak Welsh', 'belief', and finally 'bus link' tasks broadly reflect this fact. When using the Traditional Search interface, however, participants obtained the second lowest score for the health insurance task, so the correctness scores do not appear to vary simply as a function of the number of available answers. In fact, the more important factor appears to be the position of the answer in the variable list; whilst the answers chosen in the 'speak Welsh' task were right at the top, the variables relating to health insurance were much further down, and many participants simply gave up on the dataset before they got to them.

Table 1.2 shows the mean completion times for each task. A task × interface repeated measures GLM procedure shows that participants completed the task significantly faster using the Web Search interface (F~1,19~ = 18.0, p < 0.001). There was also a main effect of task (F~2,38~ = 4.1, p = 0.025, Greenhouse-Geisser correction applied). Post hoc pairwise comparisons indicate that this was due to the 'bus link' and 'health insurance' tasks taking significantly longer to complete than the 'speak Welsh' task. 

A task order × interface repeated measures GLM procedure was conducted to check for task order effects. There was a main effect of interface, showing that people completed the tasks significantly faster using the Web Search interface (F~1,16~ = 8.6, p < 0.01), but order did not have a significant effect at the 5% level (F~3,48~ = 2.2, p = 0.1), and there was no interaction effect (F~3,48~ = 0.6, p = 0.6), indicating that there was no significant difference in the rate at which participants learned to use the interfaces. 

|                | Web Search Interface|Trad. Search Interface
|  --------------| -------------------:| ---------------------:
|  Learnability  |         5.55 (0.94) |          4.05 (1.23)
|  Ease of Use   |         5.88 (0.76) |          3.70 (1.80)
|  Satisfaction   |        5.78 (0.87) |          3.15 (1.66)
[Table 1.3: Learnability: Mean (with standard deviation) ratings for overall interface learnability, ease of use and satisfaction.][Table 1.3]

### Overall Ratings

After participants had completed all the tasks using an interface, they were asked to rate on a scale of 1 to 7 its overall learnability, its overall ease of use and their overall satisfaction with it (see Table 1.3). Paired sample t-tests showed that the Web Search interface received significantly higher ratings than the Traditional Search interface for overall learnability (p = 0.002, 95% CI [0.6,2.4]), ease of use (p < 0.001, 95% CI [1.2,3.2]), and satisfaction (p < 0.001, 95% CI [1.8,3.5]). It is interesting to note that whilst there is only a 1.5 point difference between the Traditional Search interface and the Web Search interface for learnability, for ease of use this rises to 2.2 points, and for overall satisfaction 2.6 points.

### Confidence and Ease of Use Ratings for each Task

After completing each task, participants rated on a scale of 1 to 7 how confident they were that the variable they had found answered the question, and how easy it was to find the answer. 

| Task         |   Web Search Interface |  Trad. Search Interface
| -------------| ----------------------:| -----------------------:
| Belief       |            5.65 (1.90) |             2.73 (2.47)
| Welsh        |            4.78 (2.00) |             4.30 (2.62)
| Bus          |            4.85 (1.87) |             1.78 (2.26)
| Health       |            4.63 (1.99) |             2.25 (2.00)
| **Overall**  |        **4.98 (2.77)** |         **1.94 (2.34)**
[Table 1.4: Confidence: Mean (with standard deviation) confidence ratings for each task.][Table 1.4]

Table 1.4 shows the mean confidence ratings for each task. A task × interface repeated measures GLM procedure indicates that participants were significantly more confident about their answers when using the Web Search interface (F~1,19~ = 18.8, p < 0.001). Post hoc pairwise comparisons show that participants were significantly more confident about their answers in the 'speak Welsh' task than the 'bus link' or 'health insurance' tasks, and significantly less confident about their answers in the 'health insurance' task than the 'belief' task (F~2,38~ = 4.7, p = 0.015, Greenhouse-Geisser correction applied). A task × interface interaction effect (F~3,57~ = 4.4, p < 0.01) indicates that the confidence rating varied according to the interface: in the 'speak Welsh' task participants had a similar level of confidence in their answer, but for all other tasks it was much higher when using the Web Search interface.
 
Table 1.5 shows the mean ease of use ratings for each task. A task × interface repeated measures GLM procedure shows that participants found the Web Search interface significantly easier to use (F~1,19~ = 14.0, p < 0.001). There was no effect of task at the 5% probability level (F~3,57~ = 2.2, p = 0.1). 

| Task           | Web Search Interface |  Trad. Search Interface
|  ------------- |---------------------:| -----------------------:
|  Belief        |           5.08 (1.78)|              2.95 (2.39)
|  Welsh         |           4.85 (1.87)|              3.88 (2.42)
|  Bus           |           5.03 (1.92)|              1.90 (2.31)
|  Health        |           4.70 (2.11)|              2.68 (2.19)
|  **Overall**   |       **4.92 (1.92)**|          **2.85 (2.33)**
[Table 1.5: Ratings: Mean (with standard deviation) ease of use ratings for each task.][Table 1.5]

### Qualitative Feedback
Participants were asked for qualitative feedback at two points: after they had completed all the tasks with an interface, they were asked to say what they liked and disliked about it; and at the end of the study they were asked which interface they preferred and why. In addition, participants made occasional remarks about the interfaces while they were completing the tasks; these comments are also included in the analysis that follows. 

**Completed tasks & Occasional Remarks** -- Eighteen of the participants said they preferred using the Web Search interface to search for and access variables. Nine participants stated this was because it was more user-friendly or easier to use. According to p18f[^malefemale], 'It's easier to find variables and the information is clearer. In [the Traditional Search interface], the information is in another file or in another link. [In the Web Search interface] it's just there so I can see it easily.' 

[^malefemale]: Participants are referred to by p[number][sex (m/f)].

Seven participants mentioned that the search process was quicker when using the Web Search interface: 'It's so much faster. You'd just get so annoyed with [the Traditional Search interface] because of the amount of effort' (p15f); 'when I searched for something, I was able to see whether the results were relevant more immediately than with [the Traditional Search interface]' (p3f); 'when all the information came up I was able to scan it quickly and see, well this one is relevant and this one isn't (p14f).
 
Four participants described the Web Search interface as more 'intuitive'. In the words of p12m, 'the format of the site means it's more intuitive how to get around it, how to find stuff.' P2f liked the Web Search interface because, 'I could find what I was looking for.'

Seven participants commented on the simplicity of the interface. P9f said, 'It's easy because you can just search one comprehensive way rather than spending time debating which method you're going to use to actually look for your data.' P17m said this could undermine confidence in the interface, however: 'I definitely preferred [the Web Search interface], but I know this might sound weird but because it was so easy you worry that what you've done is not right, or it's not reliable.'
 
P7f said that although she preferred the output of the search process in the Web Search interface, she preferred using the catalogue search of the Traditional Search interface to specify search terms: '[the Traditional Search interface] felt a bit more open, whereas this [the Web Search interface home page] -- everything's hidden behind it. I felt happier with searching with the Traditional Search interface.' P20f, who said she preferred using the Traditional Search interface, also cited the catalogue search facility as the reason, saying it allows you to provide more details and filter the search.
 
**Post--System Interview** -- In the post-system interview, the Web Search interface received 35 positive and 17 negative comments, whilst the Traditional Search interface received 12 positive and 25 negative comments. Six participants said they found the Web Search interface easy to use, and six commented on its speed and simplicity: 'it's faster than the Traditional Search interface -- you get the same results with fewer clicks' (p13m). Two described it as 'user-friendly': '[the Web Search interface] is probably more user-friendly because the Web Search is pretty much like the Google one, so the user may be more familiar with this kind of searching method' (p11m). Two further participants compared the search facility favourably to Google and four others liked the simple, familiar format. According to p17m: 'it just seemed so easy, normal -- [an] Internet search engine but with a different purpose.' P7f disliked the simplicity of the search box, however: 'I didn't really like the fact that [the main search page] was it. I couldn't automatically do a date or a region filter.'
 
Nine participants said they liked the format of the results. According to p7f, 'the returns I got were more helpful than [with the Traditional Search interface... I think [the Web Search interface] has a better grasp of what researchers actually want, so I liked the fact that once you'd got your search returns it said what exactly was the wording of the question, and what category that came under, because sometimes a question will have a different meaning if it's asked under demographics, or asked under some other category, so you might just look and say, "that's not relevant"'. Four participants liked the fact that you could search for, or within, particular surveys and one said she liked the help video.
 
The negative comments about the Web Search interface in the interview related mainly to the description of particular variables. Four participants commented that values for some variables did not seem to be available: 'I disliked the fact that some of the variables didn't seem to have information in -- that confused me. I don't know whether that means they're searching datasets they don't have information for? That could be made clearer.' (p3f). Two participants found the appearance of many variables with the same title confusing, and p8f felt that the wording of some variable titles was unclear: 'some of the questions said things like, "Bus stop, feel, don't know"... I don't know what that question means', although she did recognise this as a potential problem with the survey, rather than the interface. Two participants commented that it was not always clear which year variables applied to, and one wondered about the geographical location of the study, which was not apparent just from looking at the variable description.

Two participants suggested that the Web Search interface returned too many variables in the search results, although 'that can be managed if you sort them according to which survey they are taken from etc.' (p13m). P7f lacked confidence in the search due to the simplicity of the interface: 'It's a little less transparent as to what's in the box... I think I'm doing the right thing but I'm not sure'. Two other participants found it hard to find the keywords to bring up the required data. One participant commented on the fact the 'back' button did not work properly and another did not like the format of the help documentation.
 
When asked what they liked about the Traditional Search interface, two key areas came up. Four participants found the extensive help documentation useful and five liked the options provided for filtering results: 'It's easier to have a general idea of categorising topics and areas... you're more likely to exclude something that is not what you want, or include what you want.' (p11m). P2f commented, 'It looked a lot more professional than [the Web Search interface]. I got the impression it had access to a lot more data'. P5f said that she found the Nesstar tool helpful.
 
When asked what they disliked about the Traditional Search interface, three participants said they found it complicated or hard to use, one described it as less intuitive than the Web Search interface and one said it was slower. Four participants said they found it difficult to get any useful results at all and eight said their searches returned too much information. P17m said, 'you felt that what you got out was quite vague, or not to the point of what you wanted. It just seemed to come up with all sorts of stuff that was completely irrelevant and just wasn't very helpful. Because it would bring up so many items you couldn't really go through them.' P3f suggested 'I think it would have been really useful, if they brought up say 200 datasets, if the variables you were actually looking for were highlighted in the small amount of text you've got underneath the heading, because then you can make a judgement.' Five participants complained about the fact that the results did not give you direct access to the variable data: 'I thought I'd worked it out then realised I hadn't. It wasn't easy going from one step to another -- it was kind of frustrating.' (p8f); 'there's too much supplementary material before you knew whether that was what you were really looking for or not' (p19f). P13m complained that there was no option for sorting the results. P7f said that it was odd that the variables search was so limited, when compared to the catalogue search: 'it just gave you a single box... it didn't give you the ability to search by region and keywords'. P9f said it was difficult to choose how to look for data: 'I'd start searching one way, then I'd think, maybe I should search that way...' 

As participants were completing the tasks, they were more likely to make negative comments than positive ones. The Web Search interface received 6 positive comments, 4 of which stated that the interface was easy to use. P19f commented on the fact that she had successfully found a variable, and p7f said that she found it helpful to be able to see what a variable contained 'without having to go into it.' The only two positive comments to be made about the Traditional Search interface also came from p7f , who said that the catalogue search seemed more efficient than the variable search, and after the second task, 'I found the searching slightly easier this time because I'd got the hang of it.'

Five of the 13 negative comments made about the Web Search interface while people were completing the tasks were due to bugs or errors, including the help video being of an inadequate resolution (one participant), the back button not working correctly (one participant), and terms in quotation marks that contained white space not being found (three participants). Two participants commented on the lack of an advanced search facility, of the type provided by the Traditional Search interface, and two disliked the fact that a search did not match only complete words (for example, a search for 'bus' would return results with 'business' in the variable title). Three participants commented on the presence of what looked like ghost variables in the results: 'it's confusing me now. I found what I think are the ones I was looking at before [in the Traditional Search interface] but when I actually click on it, it's saying that there's no source, no metadata, no value, so I don't really know whether I have found it. I still found the data easily but I've got no idea.' (p3f).
 
Of the 16 negative comments participants made while they were using the Traditional Search interface, four were people expressing their dislike for or frustration with the system: 'I am actually just getting really annoyed now' (p15f). Five participants commented that it was taking too long to find a variable, and three said they were confused or finding the process too difficult. Four participants complained about the format of the search results, including the fact that the variable search returned surveys, rather than taking you directly to the relevant variables (two participants), and the fact that certain survey years did not seem to appear in the results when they were known to exist (two participants). P7f also complained about the fact that you could not search within a survey for variables, saying that the variable lists for the datasets were 'an awful lot to try and read through'. 

# Discussion

The evaluation showed that the functionality provided by the Web Search interface was preferred to that of the Traditional Search interface for finding variables relevant to research questions. Participants were more likely to find a variable that correctly answered the question, were able to do this more quickly, and had more confidence in the results. They found the Web Search interface easier to use, and were more satisfied with the overall experience it provided.
 
The remainder of the discussion focuses on the merits of leveraging the Web Search approach to help users find variables, considering it within the context of HCIR. 

**Query Formulation and Query Reformulation** -- strive to put control of selection and interpretation of results in the user's hands. This is accomplished by allowing the user to quickly formulate and reformulate the query as their understanding of the search domain increases based on the results returned. The Web Search interface appears to support this well: 'it was simple to use, 'cause I just used keywords, and I used the same keywords in the other thing and it couldn't find it..' (p14f); and 'It's a quick way of finding what variables there are... if you were just looking at say pay, and you just wanted to look at income... I worked on a project looking at minimum wages and things like that, so we mainly use EUROSTAT, but if you could search for something...[the Web Search interface] would have been really useful for that kind of thing.' (p9f). These two examples indicate the broader feeling that Traditional Search interfaces require a more precise conceptualisation of what is required and available for search. The Web Search interface on the other-hand relies far less on the users' knowledge of the base data and so is better for variable search.

**Browsing** -- is generally considered to involve virtually no planning, preparation or focus. This kind of interaction is common in Web Search and is related to query formulation and reformulation, requiring less initial knowledge of the data available. Participants' comments suggest the interface supported this activity: 'it seemed to be an easier step between the search term and the list of variables' (p5f) ; 'When all the information came up I was able to scan it quickly, and see well this one is relevant and this one wasn't (p14f)'. Browsing for relevance appears to be key to the variable data discovery performed in the study.
 
**Faceted Search and Navigation** -- enables users to group and interact with information hierarchically, and is becoming both expected and critically important for refining search results. Participants commented that being able to limit results in the Web Search interface was useful: 'I like the different layers of options, so if I search for some different variables and some surveys clearly won't be at all relevant I can select them out.' (p3f); 'I like that you could click to look for particular surveys or particular variables. So for example if you're looking for something in Northern Ireland, I could choose NI surveys and exclude everything else so I don't get swamped with variables, because if you do type in just one thing you get an awful lot of answers coming back and you could get quite lost, so that seemed good.' (p8f). 

**Surrogates** -- these are the titles and abstracts for documents; thumbnails for Web pages; etc. which can be seen interspersed within the search results of modern Web Search interfaces. Indeed, amalgamations of surrogates can be seen in many Google searches with documents and information being displayed from 'Wikipedia' and from more general image searches. Surrogates in our Web Search interface were limited to the title of the survey. P4m commented that 'it helps to know where [the variable] is from.'

**Relevance Feedback** -- modifies an existing query based on available user-based relevance judgements for previously retrieved documents. P18f 'liked that in the title of the [variables] they have a lot of information there so it is easy to know when you have found it.' 

**Summarisation, Analytics and Visual Presentation** -- can enable users to better digest the query result, and formulate queries in a familiar interface. Indeed, we received many positive comments on this part of the interaction design: p2f said that it was 'pretty easy [to learn how to use] -- it's like a basic search engine' and that 'I like the layout; everyone knows how to use Google so everyone can find the variable they want.' P15f said that 'I liked it. It was like a Google search really. It's very familiar -- it's like Google search. You just put in all the search terms and it gives up the list, rather than having to go through all the different stages of digging through the literature.' P17m said that 'it just seemed so easy, normal -- [an] Internet search engine, but with a different purpose.'

## A 'Web Search Interface' for Research Variables
The Web Search interface was designed to simplify the process of finding and extracting variables for secondary research. Providing a familiar look, feel and functionality was a key goal of the design process. It was designed as a Web Search engine -- set within a scientific social network where users can share methods for relating, extracting and manipulating data -- to take advantage of the fact that the most familiar experience of finding information for its target users will have come from the Web.

Participants were very positive about this approach, stating explicitly that they liked the fact that it resembled a familiar Web Search engine; other participants commented on how quick, user-friendly, simple and intuitive it felt. Although both interfaces provided a single box entry system only the Web Search interface provided users with the look, feel and functionality of a search engine like Google.
 
It was not just the simplicity of the landing page that was behind the Web Search interface's success, however, but the format of the results. With the exception of the more detailed Catalogue Search, the facilities for entering search terms -- a single box -- were very similar for both interfaces. Whilst the Web Search interface presented users with a list of matching variables, however, the Traditional Search interface provided a list of surveys, with at least one further click, and possibly some scrolling, required to reach the variable of interest. For some participants, this simply made the task more time consuming. For others, it made it impossible: users expected to see what they were searching for straightaway and when they could not they assumed that something had gone wrong, either with the search process itself or the way they were using it. The Traditional Search interface was designed for the retrieval of variable data the user already knew to exist; only the Web Search interface truly supported the discovery of new data.
 
This tells us that presenting relevant results immediately after a search is very important: if a user is searching for a variable, and gets back a survey or dataset, they find this confusing. What else can we learn about the format that the results should take? It is not possible to determine from the current study whether the Web Search interface takes the optimal approach to formatting variable search results, but there is evidence that it uses at least an adequate one, as participants were able to find a variable that mostly or completely matched the search criteria the majority of the time. In fact, participants' comments indicated that they were very happy with the presentation of the results. The majority said that they liked it, and those who specified why focused on the fact that the relevance (or otherwise) of the variable could be seen at a glance. The eye tracking data shows that participants made the decision about variable relevance primarily by looking at the 'description' column of the results, so providing a summary of what the variable contains, and not just its title, appears to be important. 

![New UKDA Interface -- 'Discover' adopting the Web Search Interface and including: Faceted Search (at left); Query Reformulation (at centre); and Browsing (at centre) features.][fig:new-ukda]

[fig:new-ukda]: Figures/New-UKDA.png "New UKDA Interface."  width="75%"

## Improving the Web Search Interface

Although participants preferred the Web Search interface, it did receive some negative feedback. In several instances this was the result of a bug (for example, the back button not working properly), but it also resulted from the fact that incomplete or seemingly inaccurate variable data were sometimes returned in the results. It is likely that these problems were caused not by an error in the interface itself, but because the original format of the data in question was confusing. Nevertheless, ensuring that users fully understand the results that are returned to them, and why they appear as they do, remains a usability challenge for systems providing access to this type of data. 

##  Limitations of the Study

As the study was conducted with live Websites, it was not completely controlled. The Traditional Search interface searched a larger data catalogue than the Web Search interface, and although this meant that it potentially produced a greater number of correct answers, this did not appear to provide any advantage from the perspective of correctness scores. The time it took a search to complete varied considerably for both interfaces, from less than a second to (occasionally) more than ten. Search times were not deducted from task completion times for a number of reasons: it was not always possible to determine how long a search took (participants often opened a new window over the top to continue with the task when there was a delay); the time to access the server is a property of the system, and as such it may not be appropriate to ignore it; searches rarely took longer than a few seconds. Nevertheless, it should be noted that the task completion times recorded for the Web Search interface may rise if the proportion of the data catalogue it provides access to and/or the number of people using it increases.

A second limitation is that the study was conducted with relatively inexperienced researchers. This particular group was used because they are known to have difficulties with data discovery. There may be circumstances where the facilities provided in the Traditional Search interface are preferable to researchers with more experience, or those currently provided in the Web Search interface are not sophisticated enough.

A third limitation of the study was that it compared only two tools for accessing variable data. This study is one of the first to investigate user preferences for finding and accessing variable data; further work considering other tools or methods is undoubtedly necessary. It would also be useful to examine researchers' preferences in longitudinal, naturalistic settings, as well as controlled, laboratory-based studies. 

![Discover Results -- Search results adopting the Web Search Interface and including: Surrogates (at centre as part of 'Full Record' detail); Relevance Feedback (at top-right as 'Sorted by:'); Summarisation (at centre with each result); Analytics and Visual Presentation (at top-left) features.][fig:new-ukda-results]

[fig:new-ukda-results]: Figures/New-UKDA-Results.png "Discover Results" width="75%"


# Study Impact

Previously, researchers working with survey data from the UKDA found it difficult to discover relevant variables for analysis. These difficulties where compounded as single domain researchers became cross domain Data Scientists. To address these difficults, a Web Search interface (MethodBox) was designed as an alternative front end to the archive, enabling users to search through multiple sets of data, supporting documentation and user-contributed metadata in a single process. Since this study the main UKDA search interface has been significantly overhauled to take account of the findings (see [fig:new-ukda]) and the new way data is used and searched for. 

The new 'Discover' Variable and Question Bank interface adopted the Web Search interface paradigm described by HCIR and shown to be effective for variable search in this study. The interface implements all those features found to be useful to researchers, including: Faceted Search; Query Reformulation; Browsing; Surrogates; Relevance Feedback; Summarisation, Analytics and Visual Presentation (see [fig:new-ukda-results]). The main aim of the study was to empirically support the anecdotal preposition that Data Scientists share more in common with the 'Google Generation' than with their single-domain, single-tool forebears. We studied this in-the-wild with real applications built directly because of this anecdotal supposition; the evaluation of the MethodBox Web Search interface providing empirical support for this supposition, which has implications for scientific data search and selection more generally. We have shown that users find the Web Search engine approach intuitive and that it helps them to assemble relevant variable data for research. The findings apply not only to MethodBox but also to similar systems that support the need to search for variable data. 

The implications of this for the process of secondary data analysis are significant. Many researchers, particularly inexperienced ones, or cross disciplinarians, struggle to identify the datasets and variables they should be using to answer a research question. By enabling users to quickly search a data archive at the level of recorded factors/variables, information systems can help users to focus on research rather than the process of negotiating archives or documents. 

A straightforward means of searching provides a greater opportunity for finding relevant factors/variables that the researcher was not previously aware of. This may reduce 'investigator bias', whereby research artificially focuses on familiar datasets, but not necessarily those most relevant to the hypothesis. 

The simple provision of a Web Search interface will not ultimately eliminate the need for researchers to 'get to know' a dataset in detail, but it could make the process of data discovery quicker, easier and far less intimidating. In turn, this may generate 'digital crumbs' of metadata about the relationships between variables, users and research processes. Such metadata may eventually support crowd sourced secondary research.

# Panton Principles and the Science Code Manifesto

Science is based on building on, reusing and openly criticising the published body of scientific knowledge. For science to effectively functioning, and for society to reap the full benefits from scientific endeavours, it is crucial that scientific data be made open. In this case, we support the '[Panton Principles](http://pantonprinciples.org/)'. We further assert that 'Code is Method' and likewise support the [Science Code Manifesto](http://sciencecodemanifesto.org/). In this case, we would like to invite you to access our data and code, and question our analysis and interpretation of that data via the [full dataset, experimental protocols, and methodologies](http://wel.cs.manchester.ac.uk/data/methodbox). 

# Acknowledgements

We are grateful for the funding of this research by the Economic and Social Research Council: grant number RES-149-25-1076, 'Obesity e-Lab: e-infrastructure for interdisciplinary collaborative research', PI: Iain Buchan. We are also grateful to the UKDA and ESDSG for their direction and support for this project. 

# References


